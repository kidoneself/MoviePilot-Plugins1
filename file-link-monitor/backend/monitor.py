import time
import logging
import threading
from datetime import datetime
from pathlib import Path
from typing import List
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileCreatedEvent, FileMovedEvent

from backend.models import LinkRecord, get_session
from backend.utils.linker import FileLinker
from backend.utils.notifier import Notifier
from backend.utils.taosync import TaoSyncClient

logger = logging.getLogger(__name__)


class FileMonitorHandler(FileSystemEventHandler):
    """æ–‡ä»¶ç›‘æ§å¤„ç†å™¨"""
    
    def __init__(self, source_path: str, target_configs: List[dict], 
                 exclude_patterns: List[str], db_engine, config: dict, obfuscate_enabled: bool = False):
        self.source_path = Path(source_path)
        self.target_configs = target_configs  # [{"path": "...", "name": "..."}, ...]
        self.target_paths = [Path(t['path'] if isinstance(t, dict) else t) for t in target_configs]
        self.exclude_patterns = exclude_patterns or []
        self.db_engine = db_engine
        self.obfuscate_enabled = obfuscate_enabled
        self.linker = FileLinker(obfuscate_enabled=obfuscate_enabled)
        self.notifier = Notifier(config)
        
        # åˆå§‹åŒ–TaoSyncå®¢æˆ·ç«¯
        self.taosync_client = None
        taosync_config = config.get('taosync', {})
        if taosync_config.get('enabled'):
            self.taosync_client = TaoSyncClient(
                url=taosync_config.get('url', ''),
                username=taosync_config.get('username', 'admin'),
                password=taosync_config.get('password', ''),
                job_id=taosync_config.get('job_id', 1)
            )
            logger.info("TaoSyncå·²å¯ç”¨")
        
        # æ‰¹æ¬¡æ±‡æ€»ç›¸å…³
        self.batch_files = []  # æ‰¹æ¬¡å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
        self.last_process_time = None  # æœ€åå¤„ç†æ—¶é—´
        self.batch_timer = None  # æ±‡æ€»å®šæ—¶å™¨
        self.batch_lock = threading.Lock()  # æ‰¹æ¬¡æ•°æ®é”
        
        logger.info(f"åˆå§‹åŒ–ç›‘æ§: {source_path} -> {self.target_paths}, æ··æ·†: {obfuscate_enabled}")
    
    def on_created(self, event):
        """æ–‡ä»¶åˆ›å»ºäº‹ä»¶"""
        if event.is_directory:
            return
        
        file_path = Path(event.src_path)
        logger.info(f"æ£€æµ‹åˆ°æ–°æ–‡ä»¶: {file_path}")
        self._process_file(file_path)
    
    def on_moved(self, event):
        """æ–‡ä»¶ç§»åŠ¨äº‹ä»¶"""
        if event.is_directory:
            return
        
        file_path = Path(event.dest_path)
        logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç§»åŠ¨: {file_path}")
        self._process_file(file_path)
    
    def _process_file(self, file_path: Path):
        """å¤„ç†æ–‡ä»¶"""
        # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆ
        time.sleep(1)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not file_path.exists():
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
        
        # åªå¤„ç†è§†é¢‘æ–‡ä»¶
        from backend.utils.obfuscator import FolderObfuscator
        if not FolderObfuscator.is_video_file(file_path):
            logger.debug(f"éè§†é¢‘æ–‡ä»¶ï¼Œè·³è¿‡: {file_path}")
            return
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤
        if self.linker.should_exclude(file_path, self.exclude_patterns):
            logger.info(f"æ–‡ä»¶è¢«æ’é™¤: {file_path}")
            return
        
        # è·å–ç›¸å¯¹è·¯å¾„
        try:
            relative_path = file_path.relative_to(self.source_path)
        except ValueError:
            logger.error(f"æ–‡ä»¶ä¸åœ¨æºç›®å½•ä¸­: {file_path}")
            return
        
        # è·å–æ–‡ä»¶å¤§å°
        try:
            file_size = file_path.stat().st_size
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {e}")
            file_size = 0
        
        # ä¸ºæ¯ä¸ªç›®æ ‡ç›®å½•åˆ›å»ºç¡¬é“¾æ¥
        session = get_session(self.db_engine)
        success_count = 0
        failed_count = 0
        last_error = None
        success_targets = []
        
        try:
            for idx, target_path in enumerate(self.target_paths):
                target_file = target_path / relative_path
                
                logger.info(f"åˆ›å»ºé“¾æ¥: {file_path} -> {target_file}")
                success, method, error = self.linker.create_hardlink(file_path, target_file)
                
                if success:
                    success_count += 1
                    # è·å–ç›®æ ‡ç›®å½•çš„è‡ªå®šä¹‰åç§°
                    target_config = self.target_configs[idx]
                    target_name = target_config.get('name', target_config.get('path', str(target_path))) if isinstance(target_config, dict) else str(target_path)
                    success_targets.append(f"{target_name}: {file_path.name}")
                else:
                    failed_count += 1
                    last_error = error
                
                # è®°å½•åˆ°æ•°æ®åº“
                record = LinkRecord(
                    source_file=str(file_path),
                    target_file=str(target_file),
                    file_size=file_size,
                    link_method=method,
                    status="success" if success else "failed",
                    error_msg=error
                )
                session.add(record)
            
            session.commit()
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name}")
            
            # æ·»åŠ åˆ°æ‰¹æ¬¡æ±‡æ€»
            if success_count > 0:
                self._add_to_batch({
                    'file_name': file_path.name,
                    'targets': success_targets,
                    'time': datetime.now()
                })
            if failed_count > 0:
                self.notifier.notify_sync_failed(file_path.name, last_error or "æœªçŸ¥é”™è¯¯")
                
        except Exception as e:
            logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            session.rollback()
        finally:
            session.close()
    
    def _add_to_batch(self, file_info: dict):
        """æ·»åŠ æ–‡ä»¶åˆ°æ‰¹æ¬¡æ±‡æ€»"""
        with self.batch_lock:
            self.batch_files.append(file_info)
            self.last_process_time = datetime.now()
            
            # é‡ç½®æ±‡æ€»å®šæ—¶å™¨
            if self.batch_timer:
                self.batch_timer.cancel()
            
            # 30ç§’åæ£€æŸ¥æ˜¯å¦å‘é€æ±‡æ€»
            self.batch_timer = threading.Timer(30.0, self._check_and_send_batch)
            self.batch_timer.daemon = True
            self.batch_timer.start()
    
    def _check_and_send_batch(self):
        """æ£€æŸ¥å¹¶å‘é€æ‰¹æ¬¡æ±‡æ€»é€šçŸ¥"""
        with self.batch_lock:
            if not self.batch_files:
                return
            
            # æ£€æŸ¥æ˜¯å¦30ç§’å†…æ— æ–°æ–‡ä»¶
            if self.last_process_time and (datetime.now() - self.last_process_time).total_seconds() >= 30:
                self._send_batch_summary()
    
    def _send_batch_summary(self):
        """å‘é€æ‰¹æ¬¡æ±‡æ€»é€šçŸ¥"""
        if not self.batch_files:
            return
        
        try:
            file_count = len(self.batch_files)
            file_names = [f['file_name'] for f in self.batch_files]
            
            # å‘é€æ‰¹æ¬¡æ±‡æ€»é€šçŸ¥
            self.notifier.notify_batch_sync_success(file_names)
            
            # è§¦å‘TaoSyncåŒæ­¥
            if self.taosync_client:
                logger.info(f"æ‰¹æ¬¡å®Œæˆï¼Œè§¦å‘TaoSyncåŒæ­¥ä»»åŠ¡ï¼ˆå…±{file_count}ä¸ªæ–‡ä»¶ï¼‰")
                if self.taosync_client.trigger_sync():
                    self.notifier.notify_taosync_triggered_batch(file_count)
                else:
                    logger.error("TaoSyncè§¦å‘å¤±è´¥")
            
            logger.info(f"æ‰¹æ¬¡æ±‡æ€»é€šçŸ¥å·²å‘é€ï¼šå…±å¤„ç† {file_count} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"å‘é€æ‰¹æ¬¡æ±‡æ€»é€šçŸ¥å¤±è´¥: {e}")
        finally:
            # æ¸…ç©ºæ‰¹æ¬¡åˆ—è¡¨
            self.batch_files = []
            self.last_process_time = None


class MonitorService:
    """ç›‘æ§æœåŠ¡"""
    
    def __init__(self, config: dict, db_engine):
        self.config = config
        self.db_engine = db_engine
        self.observer = None
        self.handlers = []
        self.notifier = Notifier(config)
    
    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        monitors = self.config.get('monitors', [])
        
        if not monitors:
            logger.warning("æœªé…ç½®ç›‘æ§ç›®å½•")
            return
        
        self.observer = Observer()
        
        for monitor in monitors:
            if not monitor.get('enabled', True):
                logger.info(f"ç›‘æ§å·²ç¦ç”¨: {monitor['source']}")
                continue
            
            source = monitor['source']
            targets_config = monitor['targets']
            exclude = monitor.get('exclude_patterns', [])
            obfuscate = monitor.get('obfuscate_enabled', False)
            
            # æ£€æŸ¥æºç›®å½•æ˜¯å¦å­˜åœ¨
            if not Path(source).exists():
                logger.error(f"æºç›®å½•ä¸å­˜åœ¨: {source}")
                continue
            
            # åˆ›å»ºç›‘æ§å¤„ç†å™¨ï¼ˆä¼ é€’å®Œæ•´çš„targeté…ç½®ï¼‰
            handler = FileMonitorHandler(source, targets_config, exclude, self.db_engine, self.config, obfuscate)
            self.handlers.append(handler)
            
            # å¯åŠ¨ç›‘æ§
            self.observer.schedule(handler, source, recursive=True)
            logger.info(f"âœ… å¯åŠ¨ç›‘æ§: {source}")
        
        self.observer.start()
        logger.info("ç›‘æ§æœåŠ¡å·²å¯åŠ¨")
    
    def sync_all(self):
        """å…¨é‡åŒæ­¥æ‰€æœ‰æ–‡ä»¶"""
        logger.info("å¼€å§‹å…¨é‡åŒæ­¥...")
        
        if not self.config or 'monitors' not in self.config:
            logger.error("é…ç½®æ–‡ä»¶æ— æ•ˆ")
            return
        
        total_files = 0
        success_count = 0
        failed_count = 0
        skipped_count = 0
        
        for monitor in self.config['monitors']:
            if not monitor.get('enabled', True):
                continue
            
            source = Path(monitor['source'])
            # å…¼å®¹æ–°æ—§é…ç½®æ ¼å¼
            targets_config = monitor['targets']
            targets = []
            for t in targets_config:
                if isinstance(t, dict):
                    targets.append(Path(t['path']))
                else:
                    targets.append(Path(t))
            exclude = monitor.get('exclude_patterns', [])
            obfuscate = monitor.get('obfuscate_enabled', False)
            
            if not source.exists():
                logger.error(f"æºç›®å½•ä¸å­˜åœ¨: {source}")
                continue
            
            logger.info(f"ğŸ”„ å¼€å§‹å…¨é‡åŒæ­¥: {source}, æ··æ·†: {obfuscate}")
            
            linker = FileLinker(obfuscate_enabled=obfuscate)
            session = get_session(self.db_engine)
            
            try:
                # é€’å½’æ‰«ææ‰€æœ‰æ–‡ä»¶
                for file_path in source.rglob('*'):
                    if file_path.is_file():
                        # æ£€æŸ¥æ˜¯å¦æ’é™¤
                        if linker.should_exclude(file_path, exclude):
                            continue
                        
                        total_files += 1
                        relative_path = file_path.relative_to(source)
                        file_size = file_path.stat().st_size
                        
                        # ä¸ºæ¯ä¸ªç›®æ ‡åˆ›å»ºç¡¬é“¾æ¥
                        for target in targets:
                            target_file = target / relative_path
                            
                            # å…ˆæŸ¥æ•°æ®åº“æ˜¯å¦å·²æœ‰æˆåŠŸè®°å½•
                            existing = session.query(LinkRecord).filter(
                                LinkRecord.source_file == str(file_path),
                                LinkRecord.target_file == str(target_file),
                                LinkRecord.status == "success"
                            ).first()
                            
                            if existing:
                                logger.debug(f"æ•°æ®åº“å·²æœ‰è®°å½•ï¼Œè·³è¿‡: {target_file}")
                                skipped_count += 1
                                continue
                            
                            logger.info(f"åŒæ­¥: {file_path} -> {target_file}")
                            success, method, error = linker.create_hardlink(file_path, target_file)
                            
                            if success:
                                success_count += 1
                            else:
                                failed_count += 1
                            
                            # è®°å½•åˆ°æ•°æ®åº“
                            record = LinkRecord(
                                source_file=str(file_path),
                                target_file=str(target_file),
                                file_size=file_size,
                                link_method=method,
                                status="success" if success else "failed",
                                error_msg=error
                            )
                            session.add(record)
                
                session.commit()
                logger.info(f"âœ… å…¨é‡åŒæ­¥å®Œæˆ: æ€»æ–‡ä»¶ {total_files}, æ–°å»º {success_count}, è·³è¿‡ {skipped_count}, å¤±è´¥ {failed_count}")
                
            except Exception as e:
                logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {e}")
                session.rollback()
                return {"success": False, "message": str(e)}
            finally:
                session.close()
        
        # å‘é€å…¨é‡åŒæ­¥å®Œæˆé€šçŸ¥
        self.notifier.notify_full_sync_complete(total_files, success_count, skipped_count, failed_count)
        
        return {
            "success": True,
            "total_files": total_files,
            "success_count": success_count,
            "skipped_count": skipped_count,
            "failed_count": failed_count
        }
    
    def stop(self):
        """åœæ­¢ç›‘æ§"""
        if self.observer:
            self.observer.stop()
            self.observer.join()
            logger.info("ç›‘æ§æœåŠ¡å·²åœæ­¢")
